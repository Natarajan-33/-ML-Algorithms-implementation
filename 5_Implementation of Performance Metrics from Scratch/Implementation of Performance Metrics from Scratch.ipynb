{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa08d9fd",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "- For the given different type of result dataset we need to apply our custom build performance metrics and write down our observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0aa20",
   "metadata": {},
   "source": [
    "# PERFORMANCE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "115480e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two packages we did not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fdd21",
   "metadata": {},
   "source": [
    "# 1)Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2cb5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(df):\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for index,row in df.iterrows():                       #iterate over each row of dataframe\n",
    "        if row[\"y\"]==row[\"proba\"] and row[\"y\"]==1:        #if predicted and actual values are equal to 1, add one to TRUE POSITIVE\n",
    "            TP+=1\n",
    "        elif row[\"y\"]==row[\"proba\"] and row[\"y\"]==0:\n",
    "            TN+=1                                         # #if predicted and actual values are equal to 0, add one to TRUE NEGATIVE\n",
    "        elif row[\"y\"]!=row[\"proba\"] and row[\"y\"]==0:\n",
    "            FP+=1                                         #if predicted and actual values are NOT equal and actual value is zero, add one to FALSE POSITIVE\n",
    "        else:\n",
    "            FN+=1                                         #if predicted and actual values are NOT equal and actual value is one, add one to FALSE NEGATIVE\n",
    "    C=[TP,FP,FN,TN]\n",
    "    return np.reshape(C,(2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486ceb7",
   "metadata": {},
   "source": [
    "# 2)f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a8af86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(df):\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    pr=0        \n",
    "    re=0\n",
    "    for index,row in df.iterrows():                     #loop to calulate TRUE POSITIVE,TRUE NEGATIVE,FALSE POSITIVE,FALSE NEGATIVE\n",
    "        if row[\"y\"]==row[\"proba\"] and row[\"y\"]==1:\n",
    "            TP+=1\n",
    "        elif row[\"y\"]==row[\"proba\"] and row[\"y\"]==0:\n",
    "            TN+=1\n",
    "        elif row[\"y\"]!=row[\"proba\"] and row[\"y\"]==0:\n",
    "            FP+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "    C=[TP,FP,FN,TN]\n",
    "\n",
    "    pr=TP/(TP+FP)                #precision formula\n",
    "    re=TP/(TP+FN)                #recall formula  \n",
    "                                 \n",
    "    return (2*pr*re)/(pr+re)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4595d55",
   "metadata": {},
   "source": [
    "# 3)auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "520767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def auc(df):\n",
    "    df_sorted = df.sort_values('proba', ascending=False)      #sorting df based on probability score\n",
    "    df_sorted.reset_index(drop=True,inplace=True)\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TPR=[]\n",
    "    FPR=[]\n",
    "    actual = df_sorted['y'].tolist()\n",
    "    proba = [0]*len(actual)                               #initialise all values as zero so that we can change every value based on different threshold\n",
    "    \n",
    "    for i in tqdm(range(0,len(actual))):\n",
    "        proba[i]=1                                        #at each iteration change proba value from zero to one at each element sequently...(it is similar to changing the value based on different threshold...that is replacing every value above threshold as 1 and below threshold as 0)\n",
    "        for j in range(0,len(actual)):\n",
    "            if actual[j]==proba[j] and actual[j]==1:\n",
    "                TP+=1\n",
    "            elif actual[j]==proba[j] and actual[j]==0:\n",
    "                TN+=1\n",
    "            elif actual[j]!=proba[j] and actual[j]==0:\n",
    "                FP+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "                \n",
    "        tpr=(TP/(TP+FN))\n",
    "        fpr=(FP/(FP+TN))\n",
    "        TPR.append(tpr)\n",
    "        FPR.append(fpr)\n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0   \n",
    "        \n",
    "    return np.trapz(TPR, FPR)                             #use trapizoidal rule to calculate area under the ROC curve drawn by TPR and FPR\n",
    "                                                          #https://stackoverflow.com/questions/39537443/how-to-calculate-a-partial-area-under-the-curve-auc/39678975#39678975\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e06d6",
   "metadata": {},
   "source": [
    " # 4)Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02ce8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    pr=0\n",
    "    re=0\n",
    "    for index,row in df.iterrows():\n",
    "        if row[\"y\"]==row[\"proba\"] and row[\"y\"]==1:\n",
    "            TP+=1\n",
    "        elif row[\"y\"]==row[\"proba\"] and row[\"y\"]==0:\n",
    "            TN+=1\n",
    "        elif row[\"y\"]!=row[\"proba\"] and row[\"y\"]==0:\n",
    "            FP+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "    C=[TP,FP,FN,TN]\n",
    "    \n",
    "    return (TP+TN)/(TP+FP+FN+TN)                    #formula for accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138054f9",
   "metadata": {},
   "source": [
    "# 5)Custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aed8f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def Custom_metric(df_sorted):\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TPR=[]\n",
    "    FPR=[]\n",
    "    actual = df_sorted['y'].tolist()\n",
    "    original_proba=df_sorted[\"proba\"].tolist()                #observed value\n",
    "    new_proba = [0]*len(actual)                               #initialise all values as zero so that we can change every value based on different threshold\n",
    "    best_threshold=0\n",
    "    A=0\n",
    "    \n",
    "    for i in tqdm(range(0,len(actual)-1)):\n",
    "        \n",
    "        new_proba[i]=1                                         #at each iteration change proba value from zero to one at each element sequently...(it is similar to changing the value based on different threshold...that is replacing every value above threshold as 1 and below threshold as 0)\n",
    "        \n",
    "        for j in range(0,len(actual)-1):\n",
    "            if actual[j]==new_proba[j] and actual[j]==1:\n",
    "                TP+=1\n",
    "            elif actual[j]==new_proba[j] and actual[j]==0:\n",
    "                TN+=1\n",
    "            elif actual[j]!=new_proba[j] and actual[j]==0:\n",
    "                FP+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "                \n",
    "        𝐴=(500*FN)+(100*FP)                   #custom metric which penalises the FALSE NEGATIVE more compared to FALSE POSITIVE\n",
    "        \n",
    "        if i==0:\n",
    "            A_best=A\n",
    "            \n",
    "        if A_best>A:\n",
    "            A_best=A\n",
    "            best_threshold=original_proba[i]             #best_threshold variable stores,for which probability threshold value result in low value of our custom metric...\n",
    "            best_index=i\n",
    "            \n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "    return best_threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4dbab1",
   "metadata": {},
   "source": [
    "# 6)MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bad748f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(df_d):\n",
    "    \n",
    "    y = df_d['y'].tolist()             #converting df columns to list\n",
    "    pred = df_d['pred'].tolist()\n",
    "    \n",
    "    mse=0\n",
    "    sq_residual=0\n",
    "    \n",
    "    for i in tqdm(range(0,len(y))):\n",
    "        sq_residual+=(y[i]-pred[i])**2           #sum of squares of residual\n",
    "    mse=sq_residual/(len(y))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73556c90",
   "metadata": {},
   "source": [
    "# 7) modificed MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3de0b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MAPE(df_d):\n",
    "    \n",
    "    y = df_d['y'].tolist()\n",
    "    pred = df_d['pred'].tolist()\n",
    "    \n",
    "    mape=0\n",
    "    residual=0\n",
    "    sum_of_actual=0\n",
    "    \n",
    "    for i in tqdm(range(0,len(y))):\n",
    "        residual+=abs(y[i]-pred[i])            #absolute value of residual\n",
    "        sum_of_actual+=y[i]                    #to avoid \"divide by zero error\", we use sum of actual on the denominator\n",
    "    mape=(residual/sum_of_actual)*100\n",
    "    return mape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a69ac",
   "metadata": {},
   "source": [
    "# 8) r squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "569ac986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_sq(df_d):\n",
    "    \n",
    "    y = df_d['y'].tolist()\n",
    "    pred = df_d['pred'].tolist()\n",
    "    \n",
    "    r_sq=0\n",
    "    sum_sq_residual=0\n",
    "    total_sum_of_squares=0\n",
    "    e=0\n",
    "    \n",
    "    for i in range(0,len(y)):\n",
    "        e+=y[i]\n",
    "    y_bar=e/len(y)                 #mean of observed data\n",
    "    \n",
    "    for i in tqdm(range(0,len(y))):\n",
    "        sum_sq_residual+=(y[i]-pred[i])**2      #sum of squares of residual\n",
    "        total_sum_of_squares+=(y[i]-y_bar)**2   #total sum of squares\n",
    "        \n",
    "    R_sq=1-(sum_sq_residual/total_sum_of_squares)\n",
    "    return R_sq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3906ab",
   "metadata": {},
   "source": [
    "# 5_a1 -   Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9680d05",
   "metadata": {},
   "source": [
    "## This data has number of positive points >> number of negatives points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2cbe1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages# 5_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42a317b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a=pd.read_csv('5_a.csv')\n",
    "df_a.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25379226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.loc[(df_a.proba >= 0.5 ), 'proba'] = 1    #having threshold as 0.5 and what ever probability value greater than or equal to this is made to 1 and other to 0\n",
    "df_a.loc[(df_a.proba < 0.5 ), 'proba'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18ef9c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10000,   100],\n",
       "       [    0,     0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a5a4b",
   "metadata": {},
   "source": [
    "### observation\n",
    "- this data we can see number of positive points >> number of negatives points...\n",
    "- we can see that for the given 5_a.csv data ,all the predicted value are 1 when the threshold is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e515842",
   "metadata": {},
   "source": [
    "# 5_a2- f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8cde20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950248756218906"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49faff78",
   "metadata": {},
   "source": [
    "### observation\n",
    "- we have FN as 0 so the recall is 100%...so even though we have FP as 100 (not a 100% precision) we get good f1 score\n",
    "- for the highly imbalanced dataset and we are having only 100 FP, we are getting higher f1 score...so for this dataset we can use f1 score if we care more about recall than precision.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c9089",
   "metadata": {},
   "source": [
    "# 5_a3 - Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6b74868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10100/10100 [00:20<00:00, 497.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48829900000000004"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a=pd.read_csv('5_a.csv')     #reread the data since previouly we have modified our proba column values based on threshold\n",
    "\n",
    "auc(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a3eba",
   "metadata": {},
   "source": [
    "### observation\n",
    "- AUC tells how much the model is capable of distinguishing between classes\n",
    "- we can see that for the given prediction of all value as 1 ,we are getting lower auc(which is close to random model where auc=0.5)...when AUC is 0.5, it means the model has no class separation capacity whatsoever.so our model(auc=0.488) all so does not have much class separation capacity\n",
    "\n",
    "- for the highly imbalanced dataset and we are having 100 FP, we are getting lower auc...so we can say that for the highly imbalance data we can use auc metric. since auc even peanalise for the FP=100 when there is FN=0 and TN=0..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bfc32",
   "metadata": {},
   "source": [
    "# 5_a4 -- accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8f97bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9900990099009901"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.loc[(df_a.proba >= 0.5 ), 'proba'] = 1    #having threshold as 0.5 and what ever probability value greater than or equal to this is made to 1 and other to 0\n",
    "df_a.loc[(df_a.proba < 0.5 ), 'proba'] = 0\n",
    "\n",
    "accuracy(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f4af0",
   "metadata": {},
   "source": [
    "### observation\n",
    "- though for the highly imbalanced dataset and we are having 100 FP, we are getting higher accuracy...since accuracy not giving much weightage to class with very few datapoints....so we can say that for the highly imbalance data, we can avoid using accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c1b03",
   "metadata": {},
   "source": [
    "# 5_b1---Confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b390b6",
   "metadata": {},
   "source": [
    "###  This data has number of positive points << number of negatives points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "344d9e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  0.0  0.281035\n",
       "1  0.0  0.465152\n",
       "2  0.0  0.352793\n",
       "3  0.0  0.157818\n",
       "4  0.0  0.276648"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b=pd.read_csv('5_b.csv')\n",
    "df_b.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "231cba17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_b.loc[(df_b.proba >= 0.5 ), 'proba'] = 1         #replacing every probability score to 1 where predicted probality score is more than or equal to 0.5 and for the remening place make it to 0\n",
    "df_b.loc[(df_b.proba < 0.5 ), 'proba'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7a93f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  55,  239],\n",
       "       [  45, 9761]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a5c85",
   "metadata": {},
   "source": [
    "### observation\n",
    "- this data we can see number of positive points << number of negatives points..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6886f4",
   "metadata": {},
   "source": [
    "# 5_b2 ---f1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "399e8447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2791878172588833"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfd661",
   "metadata": {},
   "source": [
    "### observation\n",
    "- here FN and FP are high...so the recall and precision are low...so fl score is low\n",
    "- even for the highly imbalanced dataset and since recall and precision are low, we are getting low f1 score...so fl score is prefered for imbalance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2adcb",
   "metadata": {},
   "source": [
    "# 5_b3---auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb66637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10100/10100 [00:25<00:00, 398.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9377570000000001"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b=pd.read_csv('5_b.csv')\n",
    "\n",
    "auc(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d723570",
   "metadata": {},
   "source": [
    "### observation\n",
    "- AUC tells how much the model is capable of distinguishing between classes\n",
    "- here our model has 93.77% chance of predicting correcly\n",
    "- for the highly imbalanced dataset , we are getting good auc value since our model classifies positive point TP=55 even though positive class has lesser number of datapoint...so auc is prefered for imbalance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b3e21",
   "metadata": {},
   "source": [
    "# 5_b4--accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b54b03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718811881188119"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b.loc[(df_b.proba >= 0.5 ), 'proba'] = 1         #replacing every probability score to 1 where predicted probality score is more than or equal to 0.5 and for the remening place make it to 0\n",
    "df_b.loc[(df_b.proba < 0.5 ), 'proba'] = 0\n",
    "accuracy(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6bbb7",
   "metadata": {},
   "source": [
    "### observation\n",
    "- though for the highly imbalanced dataset and we are having less FP=239 and FN=45 compared to TP+TN of 10000 datapoints , we are getting higher accuracy...since accuracy not giving much weightage to class with very few datapoints....so we can say that for the highly imbalance data, we can avoid using accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc34872",
   "metadata": {},
   "source": [
    "# 5_c--custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5aa93958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.458521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.505037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.418652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.375579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y     proba\n",
       "0  0  0.458521\n",
       "1  0  0.505037\n",
       "2  0  0.418652\n",
       "3  0  0.412057\n",
       "4  0  0.375579"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c=pd.read_csv('5_c.csv')\n",
    "df_c.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe9abfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_sorted = df_c.sort_values('proba', ascending=False)\n",
    "df_c_sorted.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5857cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2851/2851 [00:02<00:00, 1048.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.230039028"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Custom_metric(df_c_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4faed",
   "metadata": {},
   "source": [
    "### observation\n",
    "- for the probability value of 0.230039028 we are getting small value for our custom metric for the given data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bac832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 969, 1020],\n",
       "       [  78,  785]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drawing confusion matric with the optimal threshold\n",
    "\n",
    "df_c_sorted.loc[(df_c_sorted.proba >= 0.230039028 ), 'proba'] = 1\n",
    "df_c_sorted.loc[(df_c_sorted.proba <0.230039028), 'proba'] = 0\n",
    "confusion_matrix(df_c_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e987f2d",
   "metadata": {},
   "source": [
    "# 5_d reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "149632a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d=pd.read_csv('5_d.csv')\n",
    "df_d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4c68a",
   "metadata": {},
   "source": [
    "# 5_d1 Mean Square Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66b5eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 157200/157200 [00:00<00:00, 1260752.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177.16569974554707"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(df_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491aa29",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- we can not interprete since the upper limit is infinity..\n",
    "- since the errors are squared before taking average ,it peanalises more if the error is larger compared to other errors(ie,outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cbcca",
   "metadata": {},
   "source": [
    "# 5_d2 MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b1867cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 157200/157200 [00:00<00:00, 1437652.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.91202994009687"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(df_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1449df",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- the average deviation between the forecasted value and actual values was 12.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21dc96",
   "metadata": {},
   "source": [
    "# 5_d3  R^2 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e76b1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 157200/157200 [00:00<00:00, 933976.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9563582786990964"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sq(df_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ed652",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- the linear regression model fits the data well and explains 95.6% variance of dependent variable...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
